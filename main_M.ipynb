{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, [1]))\n",
    "print('using GPU %s' % ','.join(map(str, [1])))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile, clever_format\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "from option import opt\n",
    "from loadData import data_pipe\n",
    "from loadData.dataAugmentation import dataAugmentation\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# MD models\n",
    "from models import S2ENet, FusAtNet, SHNet, heads, MDL\n",
    "# from models.MS2CANet import pymodel\n",
    "from models.MS2CANet2 import pymodel\n",
    "from models.CrossHL import CrossHL\n",
    "from models.HCTNet import HCTNet\n",
    "from models.DSHFNet import DSHF\n",
    "from models.MIViT import MMA\n",
    "from models import get_model_config\n",
    "\n",
    "from utils import trainer, tester, focalLoss, tools, visualation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = opt.get_args()\n",
    "# args.dataset_name = \"PaviaU\"\n",
    "args.dataset_name = \"Houston_2013\"\n",
    "\n",
    "# args.backbone = \"MDL_M\"\n",
    "# args.backbone = \"MDL_L\"\n",
    "# args.backbone = \"MDL_E_D\"\n",
    "# args.backbone = \"MDL_C\"\n",
    "\n",
    "args.backbone = \"MS2CANet\"\n",
    "# args.backbone = \"S2ENet\"\n",
    "# args.backbone = \"FusAtNet\"\n",
    "# args.backbone = \"CrossHL\"\n",
    "# args.backbone = \"HCTNet\"\n",
    "# args.backbone = \"DSHFNet\"\n",
    "# args.backbone = \"MIViT\"\n",
    "# args.backbone = \"SHNet\"\n",
    "\n",
    "args.split_type = \"disjoint\"\n",
    "get_model_config(args)\n",
    "\n",
    "print(\"args.backbone\", args.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pipe.set_deterministic(seed = 666)\n",
    "args.print_data_info = True\n",
    "args.data_info_start = 1\n",
    "args.show_gt = False\n",
    "args.remove_zero_labels = True\n",
    "\n",
    "\n",
    "if args.backbone in args.SSISO:\n",
    "    transform = dataAugmentation(args.randomCrop)   # 有些模型加增强，会造成测试精度下降很多\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "if args.dataset_name in args.SD:\n",
    "    args.train_ratio = 0.014\n",
    "    args.path_data = \"/home/icclab/Documents/lqw/DatasetSMD\"\n",
    "    # img1, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)\n",
    "    img1, img2, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)   # 为了统一\n",
    "    args.components = img1.shape[2]\n",
    "    print(img1.shape, train_gt.shape, val_gt.shape, test_gt.shape, data_gt.shape, GT.shape)\n",
    "elif args.dataset_name in args.MD:\n",
    "    args.train_ratio = 0.9\n",
    "    args.path_data = \"/home/icclab/Documents/lqw/DatasetMMF\"\n",
    "    img1, img2, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)\n",
    "    print(img1.shape, img2.shape, train_gt.shape, val_gt.shape, test_gt.shape, data_gt.shape, GT.shape)\n",
    "\n",
    "\n",
    "if args.backbone in args.MMISO or args.backbone in args.MMIMO:\n",
    "    print(\"mutlisacle multimodality\")\n",
    "    # 在这直接输出多尺度的图像\n",
    "    train_dataset = data_pipe.HyperXMM(img1, data2=img2, gt=train_gt, \n",
    "                                    transform=transform, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    test_dataset = data_pipe.HyperXMM(img1, data2=img2, gt=test_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    \n",
    "    height, wigth, data1_bands = train_dataset.data1.shape\n",
    "    height, wigth, data2_bands = train_dataset.data2.shape\n",
    "\n",
    "    # 用于 focalloss\n",
    "    train_gt_pure = train_gt[train_gt > 0] - 1\n",
    "    test_gt_pure = test_gt[test_gt > 0] - 1\n",
    "    loss_weight = focalLoss.loss_weight_calculation(test_gt_pure)\n",
    "    print(\"data1\", train_dataset.data1.shape, \"data2\", train_dataset.data2.shape)\n",
    "\n",
    "\n",
    "elif args.backbone in args.SSISO or args.backbone in args.SMIMO \\\n",
    "    or args.backbone in args.SMISO or args.backbone in args.SMIMO2 \\\n",
    "    or args.backbone in args.SMIMO3:\n",
    "\n",
    "    print(\"singlescale multimodality\")\n",
    "    train_dataset = data_pipe.HyperX(img1, data2=img2, gt=train_gt, \n",
    "                                    transform=transform, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    val_dataset = data_pipe.HyperX(img1, data2=img2, gt=val_gt, \n",
    "                                    transform=transform, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    test_dataset = data_pipe.HyperX(img1, data2=img2, gt=test_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    \n",
    "    height, wigth, data1_bands = train_dataset.data1.shape\n",
    "    height, wigth, data2_bands = train_dataset.data2.shape\n",
    "    print(\"data1\", train_dataset.data1.shape, \"data2\", train_dataset.data2.shape)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "class_num = np.max(train_gt)\n",
    "print(class_num, train_gt.shape, len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.backbone in args.MMISO or args.backbone in args.MMIMO:\n",
    "    for data11, data12, data13, data21, data22, data23, label in train_loader:\n",
    "        print(\"x.shape, y.shape\", data11.shape, data12.shape, data13.shape)\n",
    "        print(\"x.shape, y.shape\", data11.dtype, data12.dtype, data13.dtype)\n",
    "\n",
    "        print(\"x.shape, y.shape\", data21.shape, data22.shape, data23.shape)\n",
    "        print(\"x.shape, y.shape\", data21.dtype, data22.dtype, data23.dtype)\n",
    "        break\n",
    "elif args.backbone in args.SSISO or args.backbone in args.SMISO or args.backbone in args.SMIMO:\n",
    "    for data11, data12, label in train_loader:\n",
    "        print(\"x.shape, y.shape\", data11.shape, data12.shape, label.shape)\n",
    "        print(\"x.shape, y.shape\", data11.dtype, data12.dtype, label.dtype)\n",
    "        break\n",
    "elif args.backbone in args.SSSM:\n",
    "    for x, z in train_loader:\n",
    "        print(\"x.shape, y.shape\", x.shape, z.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((data21[0, 0, :, :].shape))\n",
    "# print((data22[0, 0, 3:9, 3:9].shape))\n",
    "# print((data23[0, 0, 6:12, 6:12].shape))\n",
    "# print(data21.shape)\n",
    "\n",
    "# # data21[20, 0, :, :], data22[20, 0, 3:9, 3:9], data23[20, 0, 6:12, 6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.result_dir = os.path.join(\"/home/icclab/Documents/lqw/Multimodal_Classification/PatchwiseClsFra/result\",\n",
    "                    datetime.now().strftime(\"%m-%d-%H-%M-\") + args.backbone)\n",
    "print(args.result_dir)\n",
    "\n",
    "# 加载已有权重路径\n",
    "# args.result_dir = \"/home/liuquanwei/code/DMVL_joint_MNDIS/results_final/08-09-17-05-vit_D8\"\n",
    "if not os.path.exists(args.result_dir):\n",
    "    os.mkdir(args.result_dir)\n",
    "with open(args.result_dir + '/args.json', 'w') as fid:\n",
    "    json.dump(args.__dict__, fid, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "super_head = None\n",
    "\n",
    "\n",
    "if args.backbone == \"MDL_M\":\n",
    "    model = MDL.Middle_fusion_CNN(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "    print(\"model: \", \"MDL_M\")\n",
    "\n",
    "elif  args.backbone == \"MDL_L\":\n",
    "    model = MDL.Late_fusion_CNN(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "    print(\"model: \", \"MDL_L\")\n",
    "\n",
    "elif args.backbone == \"MDL_E_D\":\n",
    "    model = MDL.En_De_fusion_CNN(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "    print(\"model: \", \"MDL_E_D\")\n",
    "\n",
    "elif  args.backbone == \"MDL_C\":\n",
    "    model = MDL.Cross_fusion_CNN(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "    print(\"model: \", \"MDL_C\")\n",
    "\n",
    "\n",
    "elif args.backbone == \"MS2CANet\":\n",
    "    FM = 64\n",
    "    args.feature_dim = 256\n",
    "    para_tune = False\n",
    "    if args.dataset_name == \"Houston_2013\":\n",
    "        para_tune = True                # para_tune 这个参数对于 Houston 的提升有两个点！！\n",
    "\n",
    "    # model = pymodel.pyCNN(data1_bands, data2_bands, classes=class_num, \\\n",
    "    #                       FM=FM, para_tune=para_tune).to(args.device)\n",
    "    # params = model.parameters()\n",
    "\n",
    "    model = pymodel.pyCNN(data1_bands, data2_bands, FM=FM, para_tune=para_tune).to(args.device)\n",
    "    super_head = heads.MS2_head(args.feature_dim, class_num=class_num).to(args.device)\n",
    "    params = list(super_head.parameters())  + list(model.parameters())\n",
    "\n",
    "elif args.backbone == 'S2ENet':\n",
    "    model = S2ENet.S2ENet(data1_bands, data2_bands, class_num, \\\n",
    "                            patch_size=args.patch_size).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"FusAtNet\":\n",
    "    model = FusAtNet.FusAtNet(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"CrossHL\":\n",
    "    FM = 16\n",
    "    model = CrossHL.CrossHL_Transformer(FM, data1_bands, data2_bands, class_num, \\\n",
    "                                        args.patch_size).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"HCTNet\":\n",
    "    model = HCTNet(in_channels=1, num_classes=class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"SHNet\":\n",
    "    FM = 64\n",
    "    # FM = 16\n",
    "    model = SHNet.SHNet(data1_bands, data2_bands, feature=FM, \\\n",
    "                        num_classes=class_num, factors=args.factors).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"DSHFNet\":\n",
    "    model = DSHF(l1=data1_bands, l2=data2_bands, \\\n",
    "                num_classes=class_num, encoder_embed_dim=64).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"MIViT\":\n",
    "    model = MMA.MMA(l1=data1_bands, l2=data2_bands, patch_size=args.patch_size, \\\n",
    "                num_patches=64, num_classes=class_num,\n",
    "                encoder_embed_dim=64, decoder_embed_dim=32, en_depth=5, \\\n",
    "                en_heads=4, de_depth=5, de_heads=4, mlp_dim=8, dropout=0.1, \\\n",
    "                emb_dropout=0.1,fusion=args.fusion).to(args.device)\n",
    "    params = model.parameters()\n",
    "    \n",
    "    loss_weight = loss_weight.to(args.device)\n",
    "    criterion = focalLoss.FocalLoss(loss_weight, gamma=2, alpha=None)\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"No models\")\n",
    "print(\"backbone: \", args.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flops, params = profile(net, inputs=(torch.randn(1, \n",
    "#                                                 args.components, \n",
    "#                                                 args.patch_size, \n",
    "#                                                 args.patch_size).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "\n",
    "# # contra_head\n",
    "# flops, params = profile(contra_head, inputs=(torch.randn(1, args.feature_dim).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "\n",
    "# # super_head\n",
    "# flops, params = profile(super_head, inputs=(torch.randn(1, args.feature_dim).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.schedule:\n",
    "\tprint(\"marker\")\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate)\n",
    "\n",
    "elif args.backbone == \"S2ENet\" \\\n",
    "\tor args.backbone == \"morphFormer\":\n",
    "\tprint(\"marker2\")\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\tscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "\n",
    "elif args.backbone == \"DBCTNet\":\n",
    "\tprint(\"marker3\")\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\tscheduler = get_cosine_schedule_with_warmup(optimizer, \\\n",
    "\t\t\t\tnum_warmup_steps = 0.1*args.epochs*len(train_loader), \\\n",
    "\t\t\t\tnum_training_steps = args.epochs*len(train_loader))\n",
    "\n",
    "else:\n",
    "\tprint(\"marker4\")\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练前加载权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.resume = os.path.join(args.result_dir, \"joint_oa_model.pth\")\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['base'], strict=False)\n",
    "    epoch_start = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {}'.format(args.resume))\n",
    "else:\n",
    "    epoch_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 999\n",
    "best_acc = 0\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "total_train_time = time.time()\n",
    "\n",
    "for epoch in range(epoch_start, args.epochs):\n",
    "    \n",
    "    if args.backbone in args.SMIMO:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_SMIMO(epoch, model, super_head, \\\n",
    "                                        criterion, train_loader, val_loader, optimizer, args)\n",
    "\n",
    "    elif args.backbone in args.SMIMO2:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_SMIMO2(epoch, model, criterion, \\\n",
    "                                        train_loader, val_loader, optimizer, args)\n",
    "        \n",
    "    elif args.backbone in args.SMIMO3:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_SMIMO3(epoch, model, criterion, \\\n",
    "                                        train_loader, val_loader, optimizer, args)\n",
    "\n",
    "    elif args.backbone in args.SMISO:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_SMISO(epoch, model, criterion, \\\n",
    "                                        train_loader, val_loader, optimizer, args)\n",
    "        \n",
    "    elif args.backbone in args.MMISO:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_MMISO(epoch, model, criterion, \\\n",
    "                                        train_loader, val_loader, optimizer, args)\n",
    "        \n",
    "    elif args.backbone in args.MMIMO:\n",
    "        train_loss, train_accuracy, test_loss, test_accuracy, train_time \\\n",
    "                                        = trainer.train_MMIMO(epoch, model, criterion, \\\n",
    "                                        train_loader, val_loader, optimizer, args)\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError(\"NO this model\")\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    if not args.schedule:\n",
    "        pass\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    \n",
    "    with open(os.path.join(args.result_dir, \"log.csv\"), 'a+', encoding='gbk') as f:\n",
    "        row=[[\"epoch\", epoch, \n",
    "            \"train loss\", train_loss, \n",
    "            \"test loss\", test_loss, \n",
    "            \"train_accuracy\", train_accuracy,\n",
    "            \"test_accuracy\", test_accuracy,\n",
    "            \"train_time\", train_time,\n",
    "            '\\n']]\n",
    "        write=csv.writer(f)\n",
    "        for i in range(len(row)):\n",
    "            write.writerow(row[i])\n",
    "\n",
    "    best_loss, best_acc = tools.save_weights(train_loss, test_loss, best_loss, best_acc, \\\n",
    "                                       test_accuracy, epoch, model, super_head, optimizer, args)\n",
    "        \n",
    "total_train_time = time.time() - total_train_time\n",
    "\n",
    "\n",
    "if super_head != None:\n",
    "    torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"super_head\": super_head.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}, \n",
    "            os.path.join(args.result_dir, \"model_last.pth\"))\n",
    "else:\n",
    "    torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}, \n",
    "            os.path.join(args.result_dir, \"model_last.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.result_dir = \"/home/leo/Multimodal_Classification/MyMultiModal/result/03-25-12-26-MIViT\"\n",
    "args.resume = os.path.join(args.result_dir, \"test_loss.pth\")\n",
    "# args.resume = os.path.join(args.result_dir, \"test_acc.pth\")\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    if super_head != None:\n",
    "        super_head.load_state_dict(checkpoint['super_head'], strict=False)\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {} epoch {}'.format(args.resume, epoch))\n",
    "else:\n",
    "    epoch_start = 0\n",
    "\n",
    "tic = time.time()\n",
    "if args.backbone in args.SSISO:\n",
    "    test_preds, targets = tester.test_SSISO(model, super_head, criterion, test_loader, args)\n",
    "if args.backbone in args.SMIMO:\n",
    "    test_preds, targets = tester.test_SMIMO(model, super_head, criterion, test_loader, args)\n",
    "if args.backbone in args.SMIMO2:\n",
    "    test_preds, targets = tester.test_SMIMO2(model, criterion, test_loader, args)\n",
    "if args.backbone in args.SMIMO3:\n",
    "    test_preds, targets = tester.test_SMIMO3(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.SMISO:\n",
    "    test_preds, targets = tester.test_SMISO(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.MMISO:\n",
    "    test_preds, targets = tester.test_MMISO(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.MMIMO:\n",
    "    test_preds, targets = tester.test_MMIMO(model, criterion, test_loader, args)\n",
    "classification, kappa = tester.get_results(test_preds, targets)\n",
    "print(classification)\n",
    "test_time = time.time() - tic\n",
    "\n",
    "with open(os.path.join(args.result_dir, \"log_final.csv\"), 'a+', encoding='gbk') as f:\n",
    "    row=[[\"training\",\n",
    "        \"\\nepoch\", epoch, \n",
    "        \"\\ndata_name = \" + str(args.dataset_name),\n",
    "        \"\\nbatch_size = \" + str(args.batch_size),\n",
    "        \"\\npatch_size = \" + str(args.patch_size),\n",
    "        \"\\nnum_components = \" + str(args.components),\n",
    "        '\\n' + classification,\n",
    "        \"\\nkappa = \\t\\t\\t\" + str(round(kappa, 4)),\n",
    "        \"\\ntotal_time = \", round(total_train_time, 2),\n",
    "        '\\ntest time = \\t' + str(round(test_time, 2)),\n",
    "        ]]\n",
    "    write=csv.writer(f)\n",
    "    for i in range(len(row)):\n",
    "        write.writerow(row[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.result_dir = \"/home/icclab/Documents/lqw/Multimodal_Classification/PatchwiseClsFra/result_Houston13/05-24-16-49-MIViT\"\n",
    "args.resume = os.path.join(args.result_dir, \"test_loss.pth\")\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    # print(\"checkpoint\", checkpoint.keys())\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    if super_head != None:\n",
    "        super_head.load_state_dict(checkpoint['super_head'], strict=False)\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {} epoch {}'.format(args.resume, epoch))\n",
    "else:\n",
    "    epoch_start = 0\n",
    "\n",
    "\n",
    "if args.backbone in args.SSISO:\n",
    "    print(\"args.randomCrop\", args.randomCrop)\n",
    "    transform = dataAugmentation(args.randomCrop)   # 有些模型加增强，会造成测试精度下降很多\n",
    "else:\n",
    "    transform = None\n",
    "\n",
    "args.print_data_info = False\n",
    "args.data_info_start = 1\n",
    "args.show_gt = False\n",
    "args.remove_zero_labels = False\n",
    "\n",
    "# create dataloader\n",
    "if args.dataset_name in args.SD:\n",
    "    args.train_ratio = 1\n",
    "    args.path_data = \"/home/icclab/Documents/lqw/DatasetSMD\"\n",
    "    # img1, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)\n",
    "    img1, img2, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)   # 为了统一\n",
    "    args.components = img1.shape[2]\n",
    "    print(img1.shape, img2.shape, train_gt.shape, val_gt.shape, test_gt.shape, data_gt.shape, GT.shape)\n",
    "elif args.dataset_name in args.MD:\n",
    "    args.train_ratio = 1\n",
    "    args.path_data = \"/home/icclab/Documents/lqw/DatasetMMF\"\n",
    "    img1, img2, train_gt, val_gt, test_gt, data_gt, GT = data_pipe.get_data(args)\n",
    "    print(img1.shape, img2.shape, train_gt.shape, val_gt.shape, test_gt.shape, data_gt.shape, GT.shape)\n",
    "\n",
    "\n",
    "if args.backbone in args.MMISO or args.backbone in args.MMIMO:\n",
    "    print(\"mutlisacle multimodality\")\n",
    "    data_dataset = data_pipe.HyperXMM(img1, data2=img2, gt=data_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    print(\"data1\", data_dataset.data1.shape, \"data2\", data_dataset.data2.shape)\n",
    "\n",
    "\n",
    "elif args.backbone in args.SSISO or args.backbone in args.SMIMO \\\n",
    "    or args.backbone in args.SMISO or args.backbone in args.SMIMO2 \\\n",
    "    or args.backbone in args.SMIMO3:\n",
    "\n",
    "    print(\"single scale multimodality\")\n",
    "    data_dataset = data_pipe.HyperX(img1, data2=img2, gt=data_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    print(\"data1\", data_dataset.data1.shape, \"data2\", data_dataset.data2.shape)\n",
    "\n",
    "\n",
    "data_loader = DataLoader(data_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "if args.backbone in args.SSISO:\n",
    "    visualation.visualation_SSISO(model, super_head, data_loader, args, groundTruth=GT, visulation=True)\n",
    "if args.backbone in args.SMIMO:\n",
    "    visualation.visualation_SMIMO(model, super_head, data_loader, args, groundTruth=GT, visulation=True)\n",
    "if args.backbone in args.SMIMO2:\n",
    "    visualation.visualation_SMIMO2(model, data_loader, args, groundTruth=GT, visulation=True)\n",
    "if args.backbone in args.SMIMO3:\n",
    "    visualation.visualation_SMIMO3(model, data_loader, args, groundTruth=GT, visulation=True)\n",
    "elif args.backbone in args.SMISO:\n",
    "    visualation.visualation_SMISO(model, data_loader, args, groundTruth=GT, visulation=True)\n",
    "elif args.backbone in args.MMISO:\n",
    "    visualation.visualation_MMISO(model, data_loader, args, groundTruth=GT, visulation=True)\n",
    "elif args.backbone in args.MMIMO:\n",
    "    visualation.visualation_MMIMO(model, data_loader, args, groundTruth=GT, visulation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.plot_loss_curve = True\n",
    "if args.plot_loss_curve:\n",
    "    # fig = plt.figure()\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # 绘制第一个数据，使用左侧y轴\n",
    "    ax1.plot(range(args.epochs), train_losses, 'blue', label=\"train_loss\")\n",
    "    ax1.plot(range(args.epochs), test_losses, 'gray', label=\"test_loss\")\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('loss', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # 创建第二个坐标轴，共享x轴但y轴在右侧\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(args.epochs), train_accuracies, 'red', label=\"train_accuracy\")\n",
    "    ax2.plot(range(args.epochs), test_accuracies, 'pink', label=\"test_accuracy\")\n",
    "    ax2.set_ylabel('accuracy', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "\n",
    "    fig.tight_layout()  # 自动调整布局以避免重叠\n",
    "\n",
    "    # # 获取两个坐标轴上的线条和标签\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc=5)\n",
    "    # plt.legend(['train_losses', 'train_accuracies', 'test_accuracies'], loc='upper right')\n",
    "    # plt.xlabel('number of training examples seen')\n",
    "    # plt.ylabel('Accuracy')\n",
    "    plt.savefig(os.path.join(args.result_dir, \"learning_curve.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for idx_o, (data, _, target) in enumerate(test_loader):\n",
    "#         target = target - 1\n",
    "#         data = data.to(args.device)\n",
    "        \n",
    "#         output = super_head(net(data))\n",
    "#         target = target.to(args.device)\n",
    "\n",
    "#         for idx, _ in enumerate(output.cpu().numpy()):\n",
    "#             # print(output.cpu().numpy().shape)\n",
    "#             if idx == 0 and idx_o == 0:\n",
    "#                 list_output = output.cpu().numpy()[idx]\n",
    "#                 list_target = target.cpu().numpy()[idx]\n",
    "#             # print(list_output.shape, list_target.shape)\n",
    "\n",
    "#             if idx < 100:\n",
    "#                 list_output = np.vstack((list_output, output.cpu().numpy()[idx]))\n",
    "#                 list_target = np.append(list_target, target.cpu().numpy()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE()\n",
    "\n",
    "# out = tsne.fit_transform(list_output)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_axis_off()\n",
    "# ax.xaxis.set_visible(False)\n",
    "# ax.yaxis.set_visible(False)\n",
    "# # fig.set_size_inches(label.shape[1] * scale / dpi, label.shape[0] * scale / dpi)\n",
    "# plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "# plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "# plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "\n",
    "# label = [\"Broccoli-weeds-1\",\"Broccoli-weeds-2\",\"Fallow\",\"Fallow-rough-plow\",\n",
    "#          \"Fallow-smooth\",\"Stubble\",\"Celery\",\"Grapes-untrained\",\n",
    "#          \"Soil-senesced-develop\",\"Corn-weeds\",\"Lettuce-4wk\",\"Lettuce-5wk\",\n",
    "#          \"Lettuce-6wk\",\"Lettuce-7wk\",\"Vinyard-untrained\",\n",
    "#          \"Vinyard-vertical-trellis\"]\n",
    "# for i in range(class_num):\n",
    "#     indices = list_target  == i\n",
    "#     x, y = out[indices].T\n",
    "#     # plt.scatter(x, y, s=5, label=label[i])\n",
    "#     plt.scatter(x, y, s=5, label=str(i+1))\n",
    "# plt.legend(loc=2,bbox_to_anchor=(1.05,1.0),borderaxespad = 0., fontsize=12)\n",
    "# plt.savefig(args.result_dir  + '/tsneFull' + '.png', dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
